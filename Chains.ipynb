{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "openai_key=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=OpenAI(openai_api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIML Robotics Engineers.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chains allows us to add functionality to the llm models also it offers multiple options for connecting multiple chains together in \n",
    "# to get more precise and accurate results from the input provided\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt=PromptTemplate.from_template(\"what could be good name for our engineering {college} for AIML\")\n",
    "chain=LLMChain(llm=client,prompt=prompt)\n",
    "chain.run(\"engineer\").strip()\n",
    "# so lwts understand how did the above code worked in an step wise manner\n",
    "# step 1 => we imported the required libraries/modules from langchain\n",
    "# step 2 => We created a prompt(in simple term a input template that's been took by the model to provide output)\n",
    "# Step 3 => Then we created a chain using LLMChain module where we provide 2 things\n",
    "# i.e the llm model's value equals to client which uses OpenAI to generate output and\n",
    "# prompt the input we want to be answered\n",
    "# step 4 => we run the chain using the \"run\" command and asked the output i the strip format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar example but by providing template in different format\n",
    "# so let;s create a prompt template first\n",
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template=\"plz suugest a {name} for my new InstaPage related to Memes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['name'], template='plz suugest a {name} for my new InstaPage related to Memes')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meme-Mantra\n"
     ]
    }
   ],
   "source": [
    "# now lets build a chain\n",
    "name_chain=LLMChain(llm=client,prompt=prompt_template_name)\n",
    "# run the chain\n",
    "print(name_chain.run(\"meaningful\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Meme-ing my way through life, one meme at a time!\"\n"
     ]
    }
   ],
   "source": [
    "name_chain=LLMChain(llm=client,prompt=prompt_template_name)\n",
    "print(name_chain.run(\"sarcastic\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleSequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now what if we want to combine multiple chains and set a sequence \n",
    "# so to set sequence we are going to use Simple Sequential chains\n",
    "# import the required modules\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# create multiple prompt_templates and their chains\n",
    "prompt_template1=PromptTemplate(\n",
    "    input_variables=['NGO_name'],\n",
    "    template=\"I want to start my NGO for {NGO_name},suggest me some innovative and inspiring name for my NGO\"\n",
    ")\n",
    "chain1=LLMChain(llm=client,prompt=prompt_template1)\n",
    "prompt_template2=PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template=\"suggest me some motivating strategies for my {name}\"\n",
    ")\n",
    "chain2=LLMChain(llm=client,prompt=prompt_template2,verbose=True)\n",
    "# now create a main chain which includes multiple sub chains in it\n",
    "main_chain=SimpleSequentialChain(chains=[chain1,chain2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3msuggest me some motivating strategies for my \n",
      "\n",
      "1. SheLeads\n",
      "2. RiseAbove\n",
      "3. GirlForce\n",
      "4. Fempower\n",
      "5. SheFluence\n",
      "6. SheSpeaks\n",
      "7. SheUnites\n",
      "8. WomenLead\n",
      "9. WomenPowered\n",
      "10. WomenRise\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Up\n"
     ]
    }
   ],
   "source": [
    "# now lets run it\n",
    "print(main_chain.run(\"women empowerment\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's understand sequential chains\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "prompt_template_name1=PromptTemplate(\n",
    "    input_variables=[\"coffee\"],\n",
    "    template=\"i want to open a cafe for {coffee}, suggest a fancy name for it\"\n",
    ")\n",
    "\n",
    "name_chain=LLMChain(llm=client, prompt=prompt_template_name1,output_key=\"cafe_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates_items=PromptTemplate( \n",
    "    input_variables=[\"cafe_name\"],\n",
    "    template=\"suggest some coffee_type for {cafe_name}\"\n",
    "    \n",
    ")\n",
    "\n",
    "food_items_chain=LLMChain(llm=client, prompt=prompt_templates_items, output_key=\"coffee_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain3=SequentialChain(chains=[name_chain, food_items_chain],\n",
    "    input_variables=[\"coffee\"],\n",
    "    output_variables=[\"cafe_name\",\"coffee_type\"]\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=chain3({\"coffee\":\"mumbai\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coffee': 'indian', 'cafe_name': '\\n\\nIndian Spice Cafe', 'coffee_type': ' Latte,\\nIrish Cream Latte,\\nMocha Latte,\\nCaramel Macchiato,\\nHazelnut Latte, \\nVanilla Latte,\\nCinnamon Dolce Latte,\\nChai Latte,\\nEspresso Con Panna,\\nHoney Vanilla Latte.'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
